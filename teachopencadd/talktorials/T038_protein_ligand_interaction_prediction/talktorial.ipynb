{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T038 · Protein Ligand Interaction Prediction\n",
    "\n",
    "**Note:** This talktorial is a part of TeachOpenCADD, a platform that aims to teach domain-specific skills and to provide pipeline templates as starting points for research projects.\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Roman Joeres, 2022, [Chair for Drug Bioinformatics, UdS and HIPS](https://www.helmholtz-hips.de/de/forschung/teams/team/wirkstoffbioinformatik/), [NextAID](https://nextaid.cs.uni-saarland.de/) project, Saarland University"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of this talktorial\n",
    "\n",
    "The goal of this talktorial is to introduce the reader to the field of protein-ligand interaction prediction using graph neural networks (GNNs). GNNs are especially useful for representing structural data such as proteins and chemical molecules (ligands) to a deep learning model. In this talktorial, we will show how to train a deep learning model to predict interactions between proteins and ligands."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "* Relevance of protein-ligand interaction prediction\n",
    "* Workflow\n",
    "* Biological background - proteins as graphs\n",
    "* Technical background\n",
    "  * Graph Isomorphism Networks\n",
    "  * Binary Cross Entropy Loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "\n",
    "* Compute graph representations\n",
    "  * Ligands to graphs\n",
    "  * Proteins to graphs\n",
    "* Data Storages\n",
    "  * Data points\n",
    "  * Data set\n",
    "  * Data module\n",
    "* Network\n",
    "  * GNN encoder\n",
    "  * Full model\n",
    "* Training routine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Theoretical background\n",
    "    * Graph Neural Networks:\n",
    "      Kipf, Welling: \"Semi-Supervised Classification with Graph Convolutional Networks\", [<i>arXiv</i> (2017)](https://arxiv.org/abs/1609.02907)\n",
    "      Bronstein, et al.: \"Geometric deep learning: going beyond Euclidean data\", [<i>IEEE Signal Processing Magazine</i> (2017), <b>4</b>, 18-42](https://doi.org/10.1109/MSP.2017.2693418)\n",
    "    * GNN-based Protein-Ligand Interaction Prediction:\n",
    "      Öztürk, et al.: \"DeepDTA: Deep drug-target binding affinity prediction\", [<i>Bioinformatics</i> (2018), <b>34</b>, i821-i829](https://doi.org/10.1093/bioinformatics/bty593)\n",
    "      Nguyen, et. al.: \"GraphDTA: Predicting drug-target binding affinity with graph neural networks\", [<i>Bioinformatics</i> (2021), <b>37</b>, 1140-1147](https://doi.org/10.1093/bioinformatics/btaa921)\n",
    "    * Graph Isomorphism Network:\n",
    "      Xu, et al.: \"How powerful are graph neural networks?\", [<i>arXiv</i> (2018)](https://arxiv.org/abs/1810.00826)\n",
    "\n",
    "* Practical background\n",
    "    * [PyTorch](https://pytorch.org/)\n",
    "    * [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/)\n",
    "    * [RDKit](http://rdkit.org/): Greg Landrum, *RDKit Documentation*, [PDF](https://www.rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf), Release on 2019.09.1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "This talktorial combines several topics, you have seen in other talktorials. Here, we will describe the general idea of how to predict interactions between proteins and ligands. If some technique used in the workflow is already presented somewhere else, I'll link to this. Otherwise, I'll explain new things below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance of protein-ligand interaction prediction\n",
    "Protein-ligand interactions are of interest in research for many reasons as can be seen in __Talktorial T016__. Drug discovery is one of the most important fields where interaction prediction between proteins and ligands has applications. In drug discovery, one wants to find a new drug for a given protein. Computer-aided interaction prediction helps in the process of virtual screening, where many possible ligands are tested _in silico_ if they interact with a certain target protein. Classically, screening of potential drugs for a target protein is done in a laboratory where the candidates are manually tested and ranked by their binding affinity. The binding affinity is a measure of how strong the interaction between two molecules is. The higher the binding affinity, the stronger the interactions and the better the two molecules bind each other.\n",
    "\n",
    "But investigating candidates manually is time-consuming and costly. Predicting binding events in a computer is way faster and cheaper. In this talktorial we will focus on predicting binding events between proteins and ligands on a qualitative level, i.e., if a protein and a ligand bind each other or not, the affinity is not interesting for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "The input for our training is a dataset comprising a set of proteins and a set of ligands and a table with binding information for every pair of proteins and ligands. We will perform supervised learning (as in __Talktorial T022__), therefore, we split the list of interactions into a training set, a validation set, and a test set. As discussed above, we will do a binary classification of interactions, i.e., does a pair of protein and molecule interact or not?\n",
    "\n",
    "The last component of our network architecture is a simple multilayer perceptron (MLP) as presented in __Talktorial T022__. The other two components are graph neural networks (GNNs) to extract features from the proteins and ligands in each pair of the dataset. As discussed in __Talktorial T035__ GNNs are used to compute a representation of graph-structured data that holds information about the structure. These representations are concatenated into one vector which serves as input for the final MLP.\n",
    "\n",
    "![Basic structure](./images/basic_structure_nn.png)\n",
    "\n",
    "*Figure 1:*\n",
    "Visualization of the model in this notebook. The shown exemplary structures are taken from the PDB entry with ID [4O75](https://www.rcsb.org/structure/4O75) (see __Talktorial T008__ for an introduction to PDB)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological background - Proteins as Graphs\n",
    "\n",
    "Here, we will focus on the conversion of proteins into graphs as the conversion of SMILES to graphs is explained in __Talktorial T033__.\n",
    "\n",
    "There are usually two ways to represent proteins in science. Either by their sequence of amino acids or as a PDB structure as introduced in __Talktorial T008__. As amino acid sequences do not contain structural information, we use PDB files of proteins as input for our structure-based models. In the graph representation of a protein, every node of the graph represents an amino acid from the protein. Edges between nodes in the graph are drawn if the two represented amino acids are within a certain distance. This is the equivalent of an interaction between the two amino acids in the protein. To compute the distance of two amino acids, we look at the coordinates of the $C_\\alpha$ atoms of the amino acids in the PDB file. If the distance between two $C_\\alpha$ atoms is below a certain distance threshold, we consider the amino acids to interact and insert an edge in the graph representation of the protein. This can be seen in Figure 2. Atoms in amino acids are enumerated. So, $C_\\alpha$ atoms of amino acids are specific carbon atoms in each amino acid that are also present in the backbone of proteins. Examples of the $C_\\alpha$ atom in exemplary amino acids can be seen in Figures 2 and 3.\n",
    "\n",
    "![Prot2Graph](./images/prot_graph_creation.png)\n",
    "\n",
    "*Figure 2:*\n",
    "Visualization of the process and idea of protein structures as graphs. For this example, we consider only the $C_\\alpha$ atoms of the cysteines to be within a distance threshold of 7 Angstrom. As both cysteines are spatially close, their sulfates generate a disulfate bridge and stabilize the protein's three-dimensions structure which is the type of interaction we want to have in the graph representations.\n",
    "\n",
    "![CAlphas](./images/calphas.jpg)\n",
    "\n",
    "*Figure 3:*\n",
    "Visualization of carbon atoms in three exemplary amino acids. Also, other numbers for atoms in amino acids are shown, but for us, only the $C_\\alpha$ atoms are interesting ([Source](https://chemistry.stackexchange.com/questions/134409/what-exactly-makes-a-carbon-atom-%CE%B1-in-a-protein-residue))."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical background\n",
    "\n",
    "In this section, we will focus on the computer science aspects of the proposed solution. Mainly, we will discuss the concrete GNN architecture and which node features we use. For simplicity (and because it works well), we will use the same network architecture to compute embeddings of kinases and their ligands."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Isomorphism Networks\n",
    "\n",
    "There is a whole zoo of GNN architectures proposed to solve many problems. If you want to get an overview of the most popular architectures, you can have a look at the [list of convolutional layers implemented in PyTorch-Geometric](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers). In this talktorial, we will use the GINConv layers as the backbone of out GNNs as they have been proven to be powerful in embedding molecular data yet remaining easy to understand in their functionality. The formula to compute an embedding of a node based on the neighbors is\n",
    "$$\\mathbf{x}^{\\prime}_i = h_{\\mathbf{\\Theta}} \\left( (1 + \\epsilon) \\cdot \\mathbf{x}_i + \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{x}_j \\right)$$\n",
    "where $\\mathcal{N}(i)$ is the set of neighbors of node $i$, $\\epsilon$ is a constant hyperparameter, and $h_{\\mathbf{\\Theta}}$ is a neural network as presented in __Talktorial T022__. The idea is to aggregate all neighbor embeddings together with the own current embedding and put this into a neural network to extract information on the nodes and their neighborhoods.\n",
    "\n",
    "As can be seen, GINConv layers do not use edge information in their computation. So, the only thing we need to extract from our proteins and ligands when turning into graphs are features for the edges. In this talktorial, we will use a very simple featurization and every node just contains categorical information on the amino acid type or atom type it represents. Information on one-hot encodings of categorical data is covered in __Talktorial T021__.\n",
    "\n",
    "The final element of our GNN module is the pooling function, which uses to compute the graph embedding based on the node embeddings in the final layer. For simplicity (and because it's surprisingly powerful) we use mean pooling! That means, we just take the mean vector over all node embeddings in the final GINConv layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Cross Entropy Loss (BCE Loss)\n",
    "\n",
    "__Talktorial T022__ introduces two loss functions, namely MSE and MAE. Both are suitable to train regression models but not appropriate for classification. For classification, there is a wide range of loss functions of which we will use the [Binary Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html).\n",
    "\n",
    "The formula to compute the loss is\n",
    "$$-\\left[ y\\cdot\\log(x)+(1-y)\\cdot\\log (1-x)\\right],$$\n",
    "where $x$ is the model output for one sample and $y$ is the label of that sample.\n",
    "\n",
    "The idea is that exactly one term of $y$ and $1-y$ equals $1$ to the formula reduces to $\\log x$ for a positive sample and $\\log (1-x)$ for a negative example. By this setting, the BCE formula ensures that you want to push the predicted values $x$ towards 0 in negative samples ($y=0$) and towards $1$ in positive cases ($y=1$).\n",
    "\n",
    "For our example, positive samples ($y=1$) are pairs of binding kinase and ligand, then $x$ should be close to 1. According to this, negative samples ($y=0$) in our example are non-binding pairs of kinase and ligand. Note the leading \"-\" in the formula, this flips the rest of the formula from a maximization problem to a minimization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical section, we will discuss every step in implementing the above-presented solution to protein-ligand interaction prediction. We will start with all the imports needed and some path definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# if sys.platform.startswith((\"linux\", \"darwin\")):\n",
    "#     !mamba install -q -y -c pyg pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joschka/mambaforge/envs/teachopencadd-t033/lib/python3.11/site-packages/chembl_webresource_client/__init__.py:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  __version__ = __import__('pkg_resources').get_distribution('chembl_webresource_client').version\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import global_mean_pool, GINConv\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import kiba_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KiBA originally contains 52498 ligands and 468 proteins.\n",
      "KiBA after dropping sparse rows contains 79 ligands and 468 proteins.\n",
      "KiBA finally contains 79 ligands and 373 proteins.\n",
      "Preprocessing ligands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joschka/teachopencadd/teachopencadd/talktorials/T038_protein_ligand_interaction_prediction/utils/kiba_analysis.py:41: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[0],\n",
      "/Users/joschka/teachopencadd/teachopencadd/talktorials/T038_protein_ligand_interaction_prediction/utils/kiba_analysis.py:43: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mol_finder.filter(chembl_id=row[0]).only([\"molecule_structures\"])\n",
      "/Users/joschka/teachopencadd/teachopencadd/talktorials/T038_protein_ligand_interaction_prediction/utils/kiba_analysis.py:41: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[0],\n",
      "/Users/joschka/teachopencadd/teachopencadd/talktorials/T038_protein_ligand_interaction_prediction/utils/kiba_analysis.py:43: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mol_finder.filter(chembl_id=row[0]).only([\"molecule_structures\"])\n",
      "/Users/joschka/teachopencadd/teachopencadd/talktorials/T038_protein_ligand_interaction_prediction/utils/kiba_analysis.py:41: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[0],\n",
      "/Users/joschka/teachopencadd/teachopencadd/talktorials/T038_protein_ligand_interaction_prediction/utils/kiba_analysis.py:43: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mol_finder.filter(chembl_id=row[0]).only([\"molecule_structures\"])\n",
      "/Users/joschka/teachopencadd/teachopencadd/talktorials/T038_protein_ligand_interaction_prediction/utils/kiba_analysis.py:41: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[0],\n",
      "/Users/joschka/teachopencadd/teachopencadd/talktorials/T038_protein_ligand_interaction_prediction/utils/kiba_analysis.py:43: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mol_finder.filter(chembl_id=row[0]).only([\"molecule_structures\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ligand availability analysis KiBA contains 76 ligands and 373 proteins.\n",
      "Preprocessing ligands finished\n",
      "Preprocessing proteins\n",
      "After protein availability analysis KiBA contains 76 ligands and 275 proteins.\n",
      "Preprocessing proteins finished\n",
      "Preprocessing interactions\n",
      "Finally, KiBA comprises 20475 interactions.\n",
      "Preprocessing interactions finished\n"
     ]
    }
   ],
   "source": [
    "HERE = Path(\"./\")\n",
    "DATA = HERE / \"data\"\n",
    "IMGS = HERE / \"images\"\n",
    "\n",
    "# This method calls a data-preprocessing pipeline that is very technical and not of bigger interest for this talktorial.\n",
    "# The method basically converts the KiBA dataset from an excel table to a dataset of structures in the format that we need.\n",
    "kiba_preprocessing(DATA / \"KIBA.csv\", DATA / \"resources\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute graph representations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ligands to graphs\n",
    "\n",
    "First, we're going to implement the conversion of ligands into graphs. For the following explanation, the ligand has $N$ atoms. To encode a graph, we have to compute a matrix of the node features (a $N\\times F$-matrix where $F$ is the number of features per node) and a matrix of the edges given by pairs of the participating node ids.\n",
    "\n",
    "Due to some PyTorch Geometric-related implementation details, the edge matrix has to have the format $2\\times N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every atom type we consider, map the symbol to a numerical value for one-hot encoding.\n",
    "atoms_to_num = dict(\n",
    "    (atom, i) for i, atom in enumerate([\"C\", \"N\", \"O\", \"F\", \"P\", \"S\", \"Cl\", \"Br\", \"I\"])\n",
    ")\n",
    "\n",
    "\n",
    "def atom_to_onehot(atom):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoding for an atom given its index in the atoms_to_num dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    atom: str\n",
    "        Atomic symbol of the atom to represent\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        A one-hot tensor encoding the atoms features.\n",
    "    \"\"\"\n",
    "    # initialize a 0-vector ...\n",
    "    one_hot = torch.zeros(len(atoms_to_num) + 1, dtype=torch.float)\n",
    "    # ... and set the according field to one, ...\n",
    "    if atom in atoms_to_num:\n",
    "        one_hot[atoms_to_num[atom]] = 1.0\n",
    "    # ... the last field is used to represent atom types that do not have their own field in the one-hot vector\n",
    "    else:\n",
    "        one_hot[len(atoms_to_num)] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    \"\"\"\n",
    "    Convert a molecule given as SDF file into a graph.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    smiles: str\n",
    "        Path to the file storing the structural information of the ligand\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor]\n",
    "        A pair of node features and edges in the PyTorch Geometric format\n",
    "    \"\"\"\n",
    "    # read in the molecule from an SDF file\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    atoms, bonds = [], []\n",
    "    # check if the molecule is valid\n",
    "    if mol is None:\n",
    "        print(smiles)\n",
    "        return None, None\n",
    "\n",
    "    # iterate over all atom, compute the feature vector and store them in a torch.Tensor object\n",
    "    for atom in mol.GetAtoms():\n",
    "        atoms.append(atom_to_onehot(atom.GetSymbol()))\n",
    "    atoms = torch.stack(atoms)\n",
    "\n",
    "    # iterate over all bonds in the molecule and store them in the PyTorch Geometric specific format in a torch.Tensor,\n",
    "    for bond in mol.GetBonds():\n",
    "        bonds.append((bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()))\n",
    "        bonds.append((bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()))\n",
    "    bonds = torch.tensor(bonds, dtype=torch.long).T\n",
    "\n",
    "    return atoms, bonds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proteins to graphs\n",
    "\n",
    "Similar to how we converted ligands to graphs, we convert proteins into graphs. The output will be the same, a pair of node features and edges. To get more information on the PDB format, read [this](https://www.cgl.ucsf.edu/chimera/docs/UsersGuide/tutorials/pdbintro.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mapping from amino acids to numbers for one-hot encoding\n",
    "aa_to_num = dict(\n",
    "    (aa, i)\n",
    "    for i, aa in enumerate(\n",
    "        [\n",
    "            \"ALA\",\n",
    "            \"ARG\",\n",
    "            \"ASN\",\n",
    "            \"ASP\",\n",
    "            \"CYS\",\n",
    "            \"GLU\",\n",
    "            \"GLN\",\n",
    "            \"GLY\",\n",
    "            \"HIS\",\n",
    "            \"ILE\",\n",
    "            \"LEU\",\n",
    "            \"LYS\",\n",
    "            \"MET\",\n",
    "            \"PHE\",\n",
    "            \"PRO\",\n",
    "            \"SER\",\n",
    "            \"THR\",\n",
    "            \"TRP\",\n",
    "            \"TYR\",\n",
    "            \"VAL\",\n",
    "            \"UNK\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def aa_to_onehot(aa):\n",
    "    \"\"\"\n",
    "    Compute the one-hot vector for an amino acid representing node.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    aa: str\n",
    "        The three-letter code of the amino acid to be represented\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        A one-hot tensor encoding the atoms features.\n",
    "    \"\"\"\n",
    "    one_hot = torch.zeros(len(aa_to_num), dtype=torch.float)\n",
    "    one_hot[aa_to_num[aa]] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def pdb_to_graph(pdb_file_path, max_dist=7.0):\n",
    "    \"\"\"\n",
    "    Extract a graph representation of a protein from the PDB file.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    pdb_file_path: str\n",
    "        Filepath of the PDB file containing structural information on the protein\n",
    "    max_dist: float\n",
    "        Distance threshold to apply when computing edges between amino acids\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor]\n",
    "        A pair of node features and edges in the PyTorch Geometric format\n",
    "    \"\"\"\n",
    "    # read in the PDB file by looking for the Calpha atoms and extract their amino acid and coordinates based on the positioning in the PDB file\n",
    "    residues = []\n",
    "    with open(pdb_file_path, \"r\") as protein:\n",
    "        for line in protein:\n",
    "            if line.startswith(\"ATOM\") and line[12:16].strip() == \"CA\":\n",
    "                residues.append(\n",
    "                    (\n",
    "                        line[17:20].strip(),\n",
    "                        float(line[30:38].strip()),\n",
    "                        float(line[38:46].strip()),\n",
    "                        float(line[46:54].strip()),\n",
    "                    )\n",
    "                )\n",
    "    # Finally compute the node features based on the amino acids in the protein\n",
    "    node_feat = torch.stack([aa_to_onehot(res[0]) for res in residues])\n",
    "\n",
    "    # compute the edges of the protein by iterating over all pairs of amino acids and computing their distance\n",
    "    edges = []\n",
    "    for i in range(len(residues)):\n",
    "        res = residues[i]\n",
    "        for j in range(i + 1, len(residues)):\n",
    "            tmp = residues[j]\n",
    "            if math.dist(res[1:4], tmp[1:4]) <= max_dist:\n",
    "                edges.append((i, j))\n",
    "                edges.append((j, i))\n",
    "\n",
    "    # store the edges in the PyTorch Geometric format\n",
    "    edges = torch.tensor(edges, dtype=torch.long).T\n",
    "\n",
    "    return node_feat, edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data storages\n",
    "\n",
    "Storing and representing input data for our neural network in protein-ligand interaction prediction is a bit different from other neural networks. Therefore, we have to define our own classes to represent the data. The main difference to training an MLP as in __Talktorial T008__, apart from graphs being the input, is that we have two data points as input. A graph of the protein and a graph of the ligand. Therefore, we need to implement out own data infrastructure.\n",
    "\n",
    "#### Data points\n",
    "\n",
    "Usually the built-in [Data](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data) class of [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) is used to represent only one graph, for our task, the data contains two graphs, therefore, we need to adapt the functionality to compute the number of nodes and edges for one data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataPair(Data):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self):\n",
    "        return self[\"lig_x\"].size(0) + self[\"prot_x\"].size(0)\n",
    "\n",
    "    @property\n",
    "    def num_edges(self):\n",
    "        return self[\"lig_edge_index\"].size(1) + self[\"prot_edge_index\"].size(1)\n",
    "\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Method that is necessary to overwrite for successful batching of DTIDataPair object.\n",
    "        In case of interest, one can look at this explanation:\n",
    "        https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
    "\n",
    "        When multiple samples are sent through a network at once, they are aggregated into batches.\n",
    "        In PyTorch Geometric this is done by copying all n graphs for one batch into one graph with\n",
    "        n connected components. Because of this, the node ids in the edge_index objects have to be\n",
    "        changed. As they have to be increased by a fixed offset based on the number of nodes in the\n",
    "        batch so far, this method computes this offset in case the edge_indices of either the\n",
    "        proteins or ligands.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        key: str\n",
    "            String name of the field of this class to increment while batching\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A one-element tensor describing how to modify the values when batching.\n",
    "        \"\"\"\n",
    "        if not key.endswith(\"edge_index\"):\n",
    "            return super().__inc__(key, value, *args, **kwargs)\n",
    "        lenedg = len(\"edge_index\")\n",
    "        prefix = key[:-lenedg]\n",
    "        return self[prefix + \"x\"].size(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data set\n",
    "\n",
    "This is where the real data magic happens. In the dataset, we read the data points and process it into the graphical representation we want to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataset(InMemoryDataset):\n",
    "    def __init__(self, folder_name, file_index):\n",
    "        self.folder_name = folder_name\n",
    "        super().__init__(root=folder_name)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[file_index])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"\n",
    "        Just store the names of the files where the training split, validation split, and test split are stored.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            A list of filenames where the preprocessed data is stored to not recompute the preprocessing every time.\n",
    "        \"\"\"\n",
    "        return [\"train.pt\", \"val.pt\", \"test.pt\"]\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        This function is called internally in the preprocessing routine of PyTorch Geometric and defined how the dataset of PDB files, ligands, and an interaction table is converted into a dataset of graphs, ready for deep learning.\n",
    "        \"\"\"\n",
    "        # compute all ligand graphs and store them as a dictionary with their names as key and the graphs as values\n",
    "        ligand_graphs = dict()\n",
    "        with open(Path(self.folder_name) / \"tables\" / \"ligands.tsv\", \"r\") as data:\n",
    "            for line in data.readlines()[1:]:\n",
    "                chembl_id, smiles = line.strip().split(\"\\t\")[:2]\n",
    "                ligand_graphs[chembl_id] = smiles_to_graph(smiles)\n",
    "\n",
    "        # compute all protein graphs and store them as a dictionary with their names as key and the graphs as values\n",
    "        protein_graphs = dict(\n",
    "            [\n",
    "                (filename[:-4], pdb_to_graph(Path(self.folder_name) / \"proteins\" / filename))\n",
    "                for filename in os.listdir(Path(self.folder_name) / \"proteins\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        with open(Path(self.folder_name) / \"tables\" / \"inter.tsv\") as inter:\n",
    "            data_list = []\n",
    "            for line in inter.readlines()[1:]:\n",
    "                # read a line with one interaction sample. Extract ligand and protein ID and get their graphs from the dictionaries above\n",
    "                protein, ligand, y = line.strip().split(\"\\t\")\n",
    "                lig_node_feat, lig_edge_index = ligand_graphs[ligand]\n",
    "                prot_node_feat, prot_edge_index = protein_graphs[protein]\n",
    "\n",
    "                # if either ligand or protein are invalid graphs, skip this sample ...\n",
    "                if lig_node_feat is None or prot_node_feat is None:\n",
    "                    print(line.strip())\n",
    "                    continue\n",
    "\n",
    "                # ... otherwise, create a datapoint using the class from above\n",
    "                data_list.append(\n",
    "                    DTIDataPair(\n",
    "                        lig_x=lig_node_feat,\n",
    "                        lig_edge_index=lig_edge_index,\n",
    "                        prot_x=prot_node_feat,\n",
    "                        prot_edge_index=prot_edge_index,\n",
    "                        y=torch.tensor(float(y), dtype=torch.float),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # shuffle the data, and compute how many samples go into which split\n",
    "            random.shuffle(data_list)\n",
    "            train_frac = int(len(data_list) * 0.7)\n",
    "            test_frac = int(len(data_list) * 0.1)\n",
    "\n",
    "            # then split the data and store them for later reuse without running the preprocessing pipeline\n",
    "            train_data, train_slices = self.collate(data_list[:train_frac])\n",
    "            torch.save((train_data, train_slices), self.processed_paths[0])\n",
    "            val_data, val_slices = self.collate(data_list[train_frac:-test_frac])\n",
    "            torch.save((val_data, val_slices), self.processed_paths[1])\n",
    "            test_data, test_slices = self.collate(data_list[-test_frac:])\n",
    "            torch.save((test_data, test_slices), self.processed_paths[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data module\n",
    "\n",
    "This is just a handy class holding all three splits of a dataset and providing data loaders for training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataModule:\n",
    "    def __init__(self, folder_name):\n",
    "        self.train = DTIDataset(folder_name, 0)\n",
    "        self.val = DTIDataset(folder_name, 1)\n",
    "        self.test = DTIDataset(folder_name, 2)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Create and return a dataloader for the training dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch_geometric.loaders.DataLoader\n",
    "            Dataloader on the training dataset\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.train, batch_size=64, shuffle=True, follow_batch=[\"prot_x\", \"lig_x\"]\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Create and return a dataloader for the validation dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch_geometric.loaders.DataLoader\n",
    "            Dataloader on the validation dataset\n",
    "        \"\"\"\n",
    "        return DataLoader(self.val, batch_size=64, shuffle=True, follow_batch=[\"prot_x\", \"lig_x\"])\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"\n",
    "        Create and return a dataloader for the test dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch_geometric.loaders.DataLoader\n",
    "            Dataloader on the test dataset\n",
    "        \"\"\"\n",
    "        return DataLoader(self.test, batch_size=64, shuffle=True, follow_batch=[\"prot_x\", \"lig_x\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "Here, we will implement the networks as defined in the theory section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNN encoder\n",
    "\n",
    "First, the GNN encoder which we will use for both, embedding proteins and embedding ligands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoding(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=64, num_layers=3):\n",
    "        \"\"\"\n",
    "        Encoding to embed structural data using a stack of GINConv layers.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        input_dim: int\n",
    "            Size of the feature vector of the data\n",
    "        hidden_dim: int\n",
    "            Number of hidden neurons to use when computing the embeddings\n",
    "        output_dim: int\n",
    "            Size of the output vector of the final graph embedding after a final mean pooling\n",
    "        num_layers: int\n",
    "            Number of layers to use when computing embedding. This includes input and output layers, so values below 3 are meaningless.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = (\n",
    "            [\n",
    "                # define the input layer\n",
    "                GINConv(\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(input_dim, hidden_dim),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(hidden_dim, hidden_dim),\n",
    "                        nn.BatchNorm1d(hidden_dim),\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "            + [\n",
    "                # define a number of hidden layers\n",
    "                GINConv(\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(hidden_dim, hidden_dim),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(hidden_dim, hidden_dim),\n",
    "                        nn.BatchNorm1d(hidden_dim),\n",
    "                    )\n",
    "                )\n",
    "                for _ in range(num_layers - 2)\n",
    "            ]\n",
    "            + [\n",
    "                # define the output layer\n",
    "                GINConv(\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(hidden_dim, hidden_dim),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(hidden_dim, output_dim),\n",
    "                        nn.BatchNorm1d(output_dim),\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward a batch of samples through this network to compute the forward pass.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            feature matrices of the graphs forwarded through the network\n",
    "        edge_index: torch.Tensor\n",
    "            edge indices of the graphs forwarded through the network\n",
    "        batch: torch.Tensor\n",
    "            Some internally used information, not relevant for the topic of this talktorial\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x=x, edge_index=edge_index)\n",
    "        pool = global_mean_pool(x, batch)\n",
    "        return F.normalize(pool, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full model\n",
    "\n",
    "Define the full model according to the workflow proposed in the theory section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTINetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # create encoders for both, proteins and ligands\n",
    "        self.prot_encoder = Encoding(21)\n",
    "        self.lig_encoder = Encoding(10)\n",
    "\n",
    "        # define a simple FNN to compute the final prediction (to bind or not to bind)\n",
    "        self.combine = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(256, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Define the standard forward process of this network.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        data: DTIDataPairBatch\n",
    "            A batch of DTIDataPair samples to be predicted to train on them\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Prediction values for all pairs in the input batch\n",
    "        \"\"\"\n",
    "        # compute the protein embeddings using the protein embedder on the protein data of the batch\n",
    "        prot_embed = self.prot_encoder(\n",
    "            x=data.prot_x,\n",
    "            edge_index=data.prot_edge_index,\n",
    "            batch=data.prot_x_batch,\n",
    "        )\n",
    "\n",
    "        # compute the ligand embeddings using the ligand embedder on the ligand data of the batch\n",
    "        lig_embed = self.lig_encoder(\n",
    "            x=data.lig_x,\n",
    "            edge_index=data.lig_edge_index,\n",
    "            batch=data.lig_x_batch,\n",
    "        )\n",
    "\n",
    "        # concatenate both embeddings and return the output of the FNN\n",
    "        combined = torch.cat((prot_embed, lig_embed), dim=1)\n",
    "        return self.combine(combined)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training routine\n",
    "\n",
    "In the training, we will use the [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) (which is a standard choice). As described above, we use the BCE loss function to compute how far off the model's predictions are. A special thing about the setup is that we only train for one epoch. This is because only in the first epoch the model shows improvements. After that, the model learned the dataset and does not improve much. But feel free to test more epochs. On average, one epoch takes around 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs=1):\n",
    "    \"\"\"\n",
    "    Implementation of the actual training routine.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_epochs: int\n",
    "        Number of epochs to train the model\n",
    "    \"\"\"\n",
    "    # load the data, model, and define the loss function\n",
    "    dataset = DTIDataModule(DATA / \"resources\")\n",
    "    model = DTINetwork()\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "    epoch_train_acc, epoch_train_loss, epoch_val_acc, epoch_val_loss = [], [], [], []\n",
    "\n",
    "    # train for num_epochs\n",
    "    for e in range(num_epochs):\n",
    "        print(f\"Epoch {e + 1}/{num_epochs}\")\n",
    "\n",
    "        # perform the actual training\n",
    "        train_loader = dataset.train_dataloader()\n",
    "        for b, data in enumerate(train_loader):\n",
    "            # compute the models predictions and the loss\n",
    "            pred = model.forward(data).squeeze()\n",
    "            loss = loss_fn(pred, data.y.squeeze())\n",
    "\n",
    "            # perform one step of backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # report some statistics on the training batch\n",
    "            pred = pred > 0.5\n",
    "            epoch_train_acc.append(sum(pred == data.y) / len(pred))\n",
    "            epoch_train_loss.append(loss.item())\n",
    "            print(\n",
    "                f\"\\rTraining step {(b + 1)}/{len(train_loader)}: Loss: {epoch_train_loss[-1]:.5f}\\tAcc: {epoch_train_acc[-1]:.5f}\",\n",
    "                end=\"\",\n",
    "            )\n",
    "\n",
    "        torch.save(model.state_dict(), DATA / f\"model_{e + 1}.pth\")\n",
    "\n",
    "        # perform validation of the last training epoch\n",
    "        val_loader = dataset.val_dataloader()\n",
    "        for b, data in enumerate(val_loader):\n",
    "            # compute the models predictions and the loss\n",
    "            pred = model.forward(data).squeeze()\n",
    "            loss = loss_fn(pred, data.y.squeeze())\n",
    "\n",
    "            # report some statistics on the validation batch\n",
    "            pred = pred > 0.5\n",
    "            epoch_val_acc.append(sum(pred == data.y) / len(pred))\n",
    "            epoch_val_loss.append(loss.item())\n",
    "            print(\n",
    "                f\"\\rValidation step {(b + 1)}/{len(val_loader)}: Loss: {epoch_val_loss[-1]:.5f}\\tAcc: {epoch_val_acc[-1]:.5f}\",\n",
    "                end=\"\",\n",
    "            )\n",
    "\n",
    "    # test the final model\n",
    "    print()\n",
    "    test_loss, test_acc = [], []\n",
    "    test_loader = dataset.test_dataloader()\n",
    "    for b, data in enumerate(test_loader):\n",
    "        # compute the models predictions and the loss\n",
    "        pred = model.forward(data).squeeze()\n",
    "        loss = loss_fn(pred, data.y.squeeze())\n",
    "\n",
    "        # report some statistics on the validation batch\n",
    "        pred = pred > 0.5\n",
    "        test_acc.append(sum(pred == data.y) / len(pred))\n",
    "        test_loss.append(loss.item())\n",
    "        print(f\"\\rTesting Loss: {test_loss[-1]:.5f}\\tAcc: {test_acc[-1]:.5f}\", end=\"\")\n",
    "    print(\n",
    "        f\"\\rTesting: Loss: {(sum(test_loss) / len(test_loss)):.5f}\\tAcc: {(sum(test_acc) / len(test_acc)):.5f}\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n",
    "\n",
    "    ax[0].plot(epoch_train_loss, c=\"b\", label=\"Train\")\n",
    "    ax[0].plot(\n",
    "        len(epoch_train_loss) - 1, sum(epoch_val_loss) / len(epoch_val_loss), \"rx\", label=\"Val\"\n",
    "    )\n",
    "    ax[0].set_ylim([0, 1])\n",
    "    ax[0].set(xlabel=\"Batches\")\n",
    "    ax[0].set_title(\"Loss\")\n",
    "\n",
    "    ax[1].plot(epoch_train_acc, c=\"b\", label=\"Train\")\n",
    "    ax[1].plot(\n",
    "        len(epoch_train_loss) - 1, sum(epoch_val_loss) / len(epoch_val_loss), \"rx\", label=\"Val\"\n",
    "    )\n",
    "    ax[1].set_ylim([0, 1])\n",
    "    ax[1].set(xlabel=\"Batches\")\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.savefig(IMGS / \"train_perf.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Validation step 64/64: Loss: 0.61401\tAcc: 0.68750\n",
      "Testing: Loss: 0.54650\tAcc: 0.76989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = train(1)\n",
    "torch.save(model.state_dict(), DATA / \"final_model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![TrainGraph](./images/train_perf.png)\n",
    "\n",
    "*Figure 3:*\n",
    "Visualization of the results of the first epoch of training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "As we can see in Figure 3, the loss decreases slightly while the accuracy is stagnating. Protein-ligand interaction prediction is a highly relevant and very complex field. Due to the complexity of the binding between proteins and ligands, e.g., which atoms of the ligand bind to which site of the protein, it is difficult to train a simple model to predict these interactions. In this talktorial, we discussed a proof of concept that is further investigated in the linked literature at the beginning of this talktorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "With this quiz, you can test if you understand the important lessons of this talktorial.\n",
    "\n",
    "1. Why do we use structural data instead of amino acid sequences and SMILES strings?\n",
    "2. How do we convert proteins into graphs? What are the essential parts of proteins we use for that?\n",
    "3. Difficult: Why do we need to implement our own class to represent data points?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('teachopencadd-t033')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "630b64ce78d00b3df6e79b584dc454f146e2cc52de13fe11b030df7884447d68"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
